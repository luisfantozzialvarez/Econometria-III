% !TeX document-id = {0be8c18c-9430-4e9a-bdd9-12beadebfebc}
% !TeX TXS-program:bibliography = txs:///biber
\documentclass[11pt]{beamer}
\uselanguage{portuguese}
\languagepath{portuguese}
\deftranslation[to=portuguese]{Theorem}{Teorema}
\deftranslation[to=portuguese]{theorem}{teorema}
\deftranslation[to=portuguese]{Example}{Exemplo}
\deftranslation[to=portuguese]{example}{exemplo}
\deftranslation[to=portuguese]{Lemma}{Lema}
\deftranslation[to=portuguese]{lemma}{Lema}
\deftranslation[to=portuguese]{Corollary}{Corolário}
\deftranslation[to=portuguese]{corollary}{corolário}
%\deftranslation[to=portuguese]{and}{e}

\usepackage[brazilian]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{color}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{subcaption}
%\usepackage{appendixnumberbeamer}

\newenvironment{transitionframe}{
	\setbeamercolor{background canvas}{bg=yellow}
	\begin{frame}}{
	\end{frame}
}
\usetheme{default}
\usefonttheme{structuresmallcapsserif}

%% I use a beige off white for my background
\definecolor{MyBackground}{RGB}{255,253,218}
\useinnertheme[shadow]{rounded}
\setbeamercolor{block title}{bg=MyBackground}
\setbeamercolor{block body}{bg=MyBackground}
\setbeamercolor{example title}{bg=MyBackground}
\setbeamercolor{example body}{bg=MyBackground}


\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\purple}[1]{\textcolor{purple}{#1}}
\newcommand{\gray}[1]{\textcolor{gray}{#1}}
\setbeamertemplate{navigation symbols}{}
%\setbeamertemplate{page number in head/foot}[appendixframenumber]

%\usepackage{graphics}
\usepackage{graphicx}

\definecolor{blue_emph}{RGB}{0,114,178}
\definecolor{red}{RGB}{213,94,0}
\definecolor{yellow}{RGB}{240,228,66}
\definecolor{green}{RGB}{0,158,115}
\definecolor{purple}{RGB}{204,121,167}
\definecolor{orange}{RGB}{230,159,0}
\definecolor{lightblue}{RGB}{86,180,233}

%\setbeamercolor{frametitle}{fg=blue}
%\setbeamercolor{title}{fg=blue}
\setbeamertemplate{footline}[frame number]
\setbeamertemplate{navigation symbols}{} 
\setbeamertemplate{itemize items}{-}
%\setbeamercolor{itemize item}{fg=blue}
%\setbeamercolor{itemize subitem}{fg=blue}
\setbeamertemplate{enumerate items}[default]
%\setbeamercolor{enumerate subitem}{fg=blue}
\setbeamercolor{button}{bg=MyBackground,fg=blue}
\usefonttheme{structuresmallcapsserif}

%\setbeamercolor{section in toc}{fg=blue}
%\setbeamercolor{subsection in toc}{fg=red}
\setbeamersize{text margin left=1em,text margin right=1em} 


\usepackage{appendixnumberbeamer}

\usepackage[
backend=biber,
style=authoryear,
natbib=true
]{biblatex}
\addbibresource{../bibliography.bib}

\newenvironment{wideitemize}{\itemize\addtolength{\itemsep}{10pt}}{\enditemize}
\newenvironment{wideenumerate}{\enumerate\addtolength{\itemsep}{10pt}}{\endenumerate}
\newenvironment{halfwideitemize}{\itemize\addtolength{\itemsep}{0.5em}}{\enditemize}
\newenvironment{halfwideenumerate}{\enumerate\addtolength{\itemsep}{0.5em}}{\endenumerate}


\author{Luis A. F. Alvarez}
\title{EAE1223: Econometria III}
\subtitle{Aula 5 - Metodologia de Box-Jenkins}
%\logo{}
%\institute{}
\date{\today}
%\subject{}
%\setbeamercovered{transparent}

\begin{document}

\begin{frame}[plain]
	\maketitle
\end{frame}

\begin{frame}{A metodologia}
	\begin{halfwideitemize}
		\item A metodologia de Box-Jenkins consiste numa série de etapas para estimar um modelo {\color{blue}univariado} de {\color{blue}previsão}.
		\begin{halfwideitemize}
			\item Ideia é estimar um modelo simples, embora flexível, aos dados.
		
		\end{halfwideitemize}
	\item Trata-se da metodologia básica de previsão em séries de tempo. 
\begin{halfwideitemize}
	\item Diversas metodologias modernas incorporam o ``espírito'' de Box-Jenkins.
	\item \textit{Benchmark} para avaliar outros modelos de previsão.
\end{halfwideitemize}
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Etapas da metodologia}
	\begin{halfwideitemize}
			\item A metodologia consiste de quatro etapas:
	\begin{halfwideenumerate}
		\item {\color{blue}Identificação}: nessa etapa, avaliamos os dados e identificamos quais modelos são candidatos plausíveis para reproduzir os dados.
		\item {\color{blue} Estimação}: nessa etapa, estimamos os modelos candidatos.
		\item {\color{blue} Diagnóstico:} nessa etapa, avaliamos quais dos modelos se saíram melhor, de acordo com alguns critérios.
		\item {\color{blue} Previsão:} por fim, realizamos a previsão de acordo com nosso modelo.
	\end{halfwideenumerate}
	\item Nesta aula, discutiremos cada uma dessas etapas.
	\item Começaremos revisando a classe de modelos estudadas na metodologia de Box-Jenkins.
\end{halfwideitemize}
\end{frame}




\begin{transitionframe}
	\begin{center}
		{\Huge   Modelos considerados}
	\end{center}
\end{transitionframe}

\begin{frame}{MA(q)}
	\begin{halfwideitemize}
		\item Dizemos que uma série de tempo $\{Y_t\}$ segue um MA(q) se:
		\begin{equation*}
			Y_t =\alpha + \epsilon_t + \sum_{j=1}^q \theta_j \epsilon_{t-j} 
		\end{equation*} 
		onde $\{\epsilon_t\}$ é um ruído branco.
		\item Série hoje depende diretamente da realização atual e das últimas $q$ realizações de um ruído branco.
		\item \textbf{Todo} processo de média móvel é fracamente estacionário.
		\begin{halfwideitemize}
			\item De fato, $\mathbb{E}[Y_t] = \alpha$, $\mathbb{V}[Y_t] =\left(1+\sum_{j=1}^q \theta_j^2\right) \sigma^2_\epsilon$, $\text{cov}(Y_t, Y_{t-s})= (\theta_s +\sum_{j=s+1}^q \theta_j \theta_{j-s})\sigma^2_\epsilon$ se $s\leq q$ e $0$ do contrário. Correlação morre após $q$ períodos.
		\end{halfwideitemize}
		\item Um processo MA(q) é dito \textbf{invertível} se pode ser escrito como:
		\begin{equation*}
			Y_t = \omega + \sum_{j=1}^\infty \psi_j Y_{t-j} + \epsilon_t
		\end{equation*}
		(MA(q) pode ser representado como AR($\infty$)).
	\end{halfwideitemize}
\end{frame}
\begin{frame}{AR(p) estacionário}
	\begin{halfwideitemize}
		\item Dizemos que uma série de tempo $\{Y_t\}$ segue um AR(p) estacionário se ela se escreve como:
		\begin{equation*}
			Y_t =\alpha + \sum_{j=1}^p \beta_j Y_{t-j} + \epsilon_t
		\end{equation*} 
		onde $\{\epsilon_t\}$ é um ruído branco e os coeficientes $\beta_j$ são tais que o processo resultante é fracamente estacionário.
		\item Série hoje depende diretamente das realizações passadas nos últimos $p$ períodos, mais um ruído branco.
		\begin{halfwideitemize}
			\item Mas persistência não é tão grande, de modo que a série é estacionária (não há raiz unitária).
		\end{halfwideitemize}
		\item Recorde-se que um AR(p) é estacionário se, e somente se, ele se escreve como um MA($\infty$):
		\begin{equation*}
			Y_t = \kappa + \epsilon_{t} +\sum_{j=1}^\infty \tau_j \epsilon_{t-j} 
		\end{equation*}
	\end{halfwideitemize}
\end{frame}



\begin{frame}{ARMA(p,q) estacionário}
	\begin{halfwideitemize}
		\item Dizemos que uma série de tempo $\{Y_t\}$ segue um ARMA(p,q) estacionário se:
		\begin{equation*}
			Y_t =\alpha + \sum_{i=1}^p \beta_i Y_{t-i} + \epsilon_t + \sum_{j=1}^q \theta_j \epsilon_{t-j} 
		\end{equation*} 
		onde $\{\epsilon_t\}$ é um ruído branco, e os $\beta_i$ são tais que o processo resultante é estacionário.
		\item Combinação dos dois modelos anteriores.
		\item {\color{blue} Metodologia de Box-Jenkins visará a estimar modelos na classe ARMA(p,q).}
	\end{halfwideitemize}
\end{frame}

\begin{frame}{E se os dados forem não estacionários?}
	\begin{halfwideitemize}
		\item Até agora, discutimos a modelagem supondo as séries estacionárias.
		\begin{halfwideitemize}
			\item Como fazer a previsão em casos não estacionários?
		\end{halfwideitemize}
		\item Se as séries apresentarem uma tendência estocástica, trabalhamos com os dados em primeira diferença $\{\Delta Y_t\}$. 
		
		\item Conduzidas as etapas da metodologia Box-Jenkins com os dados diferenciados, e encontradas projeções para $\Delta Y_t$ fora da amostra, recompomos as projeções  em nível usando o fato de que $Y_{t+1} = Y_t + \Delta Y_{t+1}$.
		\begin{halfwideitemize}
			\item Isto é, se temos $T$ observações, projetamos $\widehat{Y_{T+1}} = Y_T + \widehat{{\Delta Y_{T+1}}}$,  $Y_{T+2} = Y_T + \widehat{\Delta Y_{T+1}} + \widehat{\Delta Y_{T+2}}$, e assim por diante.
		\end{halfwideitemize}
		\end{halfwideitemize}
\end{frame}
\begin{frame}{Modelos ARIMA(p,d,q)}
	\begin{halfwideitemize}
				\item Em outras palavras, no caso de raiz unitária, a metodologia irá estimar um modelo {\color{blue}ARIMA(p,1,q)}, da forma:
		\begin{equation*}
			\Delta Y_t =\alpha + \sum_{i=1}^p \beta_i \Delta Y_{t-i} + \epsilon_t + \sum_{j=1}^q \theta_j \epsilon_{t-j}
		\end{equation*}
		\item De modo geral, se a série é I(d), a modelagem considerará modelos {\color{blue}ARIMA(p,d,q)}:
		$$\Phi(L)(1-L)^d Y_t =  \alpha + \Psi(L)\epsilon_t \, ,$$
		para polinômios $\Phi(L)$ e $\Psi(L)$ de grau $p$ e $q$, respectivamente.

	\end{halfwideitemize}
\end{frame}
\begin{frame}{Previsão com tendência determinística}
	\begin{halfwideitemize}
				\item Se séries apresentam tendência determinística, conduzimos a metodologia com dados \textit{detrended}. Calculadas as projeções \textit{detrended}, recompomos projeções em nível usando a tendência estimada.
				
				\item Por exemplo, se estimamos uma tendência linear:
				$$Y_t = \tilde{a} + \tilde{b}t + \tilde{\xi}_t$$
				e ajustamos um modelo ARMA  para $\tilde{\xi}_t$, a projeção para fora da amostra é:
				
				$$\widehat{Y_{T+h}} = \tilde{a} + \tilde{b}(T+h) + \widehat{\tilde{\xi}_{T+h}}\, ,$$
				onde $\widehat{\tilde{\xi}_{T+h}}$ é a projeção do ARMA para $T+h$ (veremos como calculá-la na etapa de previsão).
	\item Estimação de ARMA para dado \textit{detrended} é equivalente a estimar um modelo ARMA(p,q) com tendência determinística.
$$\Phi(L) Y_t =  \alpha + \gamma t + \Psi(L)\epsilon_t \, .$$
	\end{halfwideitemize}
\end{frame}

\begin{frame}{``Filosofia'' da metodologia de Box-Jenkins}
	\begin{halfwideitemize}
		\item A restrição a modelos ARMA(p,q) pode ser entendida a partir do {\color{blue}teorema de decomposição de Wold.}
		\item Segundo esse teorema, \textbf{qualquer processo fracamente estacionário} pode ser representado pela soma de um MA($\infty$), acrescido de uma função determinística dos $Y$ no passados:
		$$Y_t = \epsilon_t  +\sum_{l=0}^\infty \psi_l \epsilon_{t-l} + \kappa_t \, ,$$
		onde $\kappa_t$ é função ``aproximadamente'' linear de $Y_{t{\color{red}-1}}$, $Y_{t-2}$\ldots 
			\item Ideia de Box-Jenkins é aproximar essa representação por um ARMA(p,q), com \textit{p} e \textit{q} {\color{blue}pequenos}.
		\begin{halfwideitemize}
			\item Ideia é que aproximação {\color{blue}parcimoniosa}, por ser menos ruidosa, tende a funcionar melhor que modelos muito complexos.
		\end{halfwideitemize}	
	\end{halfwideitemize}
\end{frame}

\begin{transitionframe}
	\begin{center}
		{\Huge   Identificação}
	\end{center}
\end{transitionframe}

\begin{frame}{Identificação de um ARMA(p,q)}
	\begin{wideitemize}
		\item A etapa de identificação da metodologia Box-Jenkins consiste em encontrar quais modelos da classe ARMA(p,q) melhor caracterizam a série de interesse.
		\item A identificação consiste em analisar a função de autocorrelação (FAC) e a função de autocorrelação parcial (FACP) da série estacionária.

	
		
	\end{wideitemize}
\end{frame}

\begin{frame}{Função de autocorrelação serial (FAC)}
	\begin{halfwideitemize}
					\item A {\color{blue}função de autocorrelação (FAC)} de uma série $\{Y_t\}_t$ estacionária é o mapa que associa, a cada número $k > 0$, a autocorrelação de ordem $k$, i.e. $\gamma_k = \text{cor}(Y_t, Y_{t-k})$.
					\item Note que essa função está bem definida para processos estacionários, visto que $\text{cor}(Y_t, Y_{t-k})$ não depende de $t$.
					
					$$\operatorname{cor}(Y_t,Y_{t-k}) = \frac{\operatorname{cov}(Y_t, Y_{t-k})}{\operatorname{sd}(Y_t)\operatorname{sd}(Y_{t-k})} = \frac{\operatorname{cov}(Y_t, Y_{t-k})}{\mathbb{V}(Y_t)}$$
				\end{halfwideitemize}
\end{frame}

\begin{frame}{Função de autocorrelação serial parcial (FACP)}
	\begin{halfwideitemize}
		\item A {\color{blue}função de autocorrelação parcial (FACP)} de uma série $\{Y_t\}_t$ estacionária é o mapa que associa, a cada número $k > 0$, a correlação $\theta_k$ entre $Y_{t}$ e $Y_{t-k}$, {\color{blue}controlando por $Y_{t-1}, Y_{t-2}, \ldots Y_{t-k+1}$}.

	
		

	
		\item A autocorrelação parcial de ordem $k$ ($\theta_k$) é dada pelo coeficente $\tau_k$ associado a $Y_{t-k}$ no modelo preditivo linear:
	\begin{equation}
		\label{pred_model}
		\begin{aligned}
			Y_t = \tau_0 + \tau_1 Y_{t-1} + \tau_2 Y_{t-2} \ldots + \tau_k Y_{t-k} + \nu_t \\
			\mathbb{E}[\nu_t] = 0, \mathbb{E}[\nu_t Y_{t-j}] = 0,\quad j=1,\ldots k
		\end{aligned}
	\end{equation}
	\item Note que função está bem definida para processos estacionários, visto que coeficientes do melhor preditor linear em \eqref{pred_model} não dependem de $t$.
		\end{halfwideitemize}
\end{frame}
\begin{frame}{FAC e FACP estimadas}
	\begin{halfwideitemize}
		\item Na prática, não observamos a FAC nem a FACP de um processo, mas podemos estimá-las usando as realizações da série de interesse.
		\begin{halfwideitemize}
			\item Estimamos a FAC calculando as autocorrelações nos dados.
			
			$$\hat{\gamma}_k = \frac{\sum_{t=1}^{T-k} (Y_{t}-\bar{y})(Y_{t+k}-\bar{y})}{\sum_{t=1}^{T}(Y_t-\bar{y})^2} \, ,$$
			onde $\bar{y} = T^{-1}\sum_{t=1}^T Y_t$.
			\item Estimamos a FACP ajustando o modelo \eqref{pred_model} aos dados.
		\end{halfwideitemize}
	
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Inferência sobre FAC e FACP populacionais}
	\begin{halfwideitemize}
				\item Como observamos apenas algumas realizações do processo, gostaríamos de testar hipóteses sobre a FAC e FACP populacionais.
		\item Sob algumas condições mais estringentes, o intervalo $[-2/\sqrt{T}, 2/\sqrt{T}]$ é uma região de aceitação aproximadamente válida para o teste da nula $\gamma_k=0$ ($\theta_k =0$ na FACP)  contra a alternativa bilateral ao nível de significância de 5\%, com base na estatística $\hat \gamma_k$ ($\hat \theta_k$).
		\begin{halfwideitemize}
			\item São esses intervalos que são apresentados, no R, quando computamos a FAC e FACP.
			\item Para autocorrelações (parciais) estimadas que excedem esses limites, rejeitamos a hipótese nula de não autocorrelação (parcial) a essa ordem.
		\end{halfwideitemize}
	
	\end{halfwideitemize}

\end{frame}
\begin{frame}{FAC e FACP amostrais de $\Delta \text{Desemprego}$}
	\begin{figure}
		\includegraphics[scale=0.39]{graficos/acf_unemp.pdf} 		\includegraphics[scale=0.39]{graficos/pacf_unemp.pdf}
	\end{figure}
\end{frame}

\begin{frame}{Inferência conjunta sobre a FAC}
	\begin{itemize}
	 	\item Para testar a nula conjunta de que $\gamma_{1} = \gamma_{2}=\ldots = \gamma_{s} = 0$, onde $s$ é pequeno relativamente a $T$, podemos usar a {\color{blue}estatística de Ljung-Box}:
		
		$$\hat{Q}= T(T+2)\sum_{r=1}^s \frac{\hat{\gamma}_r^2}{T-r}\, .$$
		\item Com $T$ grande, sob a nula, $\hat{Q}$ segue uma qui-quadrado com $s$ graus de liberdade. Valores altos da estatística são evidência contra a nula, i.e. evidência de que ao menos uma autocorrelação entre as testadas é diferente de zero.
	\end{itemize}
\end{frame}



\begin{frame}{FAC e FACP de um AR(p) estacionário}
	\begin{halfwideitemize}
		\item Como vimos em aula anterior, a FAC de um AR(1) estacionário é dada por:
		\begin{equation*}
			\gamma_k = \text{cor}(Y_t, Y_{t-k}) = \beta_1^k, \quad |\beta_1| < 1
		\end{equation*}
		isto é, a FAC apresenta decaimento geométrico em direção a zero.
		\item De modo geral, a FAC de um AR(p) estacionário apresenta {\color{blue}decaimento em direção a zero}, visto que um AR(p) estacionário pode ser escrito como um MA($\infty$), i.e.
		\begin{equation*}
			Y_t = \mu + \epsilon_t + \sum_{j=1}^\infty  \omega_j \epsilon_{t- j}
		\end{equation*}
		\item E a FACP de um AR(p) estacionário? Pela definição da FACP de ordem $k$ como o coeficiente de $Y_{t-k}$ na regressão populacional de $Y_t$ em $Y_{t-1}$, $Y_{t-2}$,\ldots $Y_{t-k}$, esperamos que a FACP seja {\color{blue} truncada em $p$}, visto que o processo só depende diretamente das $p$ primeiras defasagens.
	\end{halfwideitemize}
\end{frame}
\begin{frame}{Exemplo: FAC e FACP estimadas de um AR(2) (T=10.000)}
	\begin{figure}
		\includegraphics[scale=0.33]{graficos/ar2_fac.pdf} 		\includegraphics[scale=0.33]{graficos/ar2_facp.pdf}
	\end{figure}
\end{frame}
\begin{frame}{FAC e FACP de um MA(q) invertível}
	\begin{halfwideitemize}
		\item Como vimos em aula anterior, a FAC de um MA(q) é {\color{blue}truncada} em $q$, visto que a correlação morre após $q$ períodos.
		\item E a FACP? Se o processo MA for invertível, vimos que ele pode ser escrito como um $AR(\infty)$. Dessa representação, fica claro que a FACP de um MA(q) apresenta {\color{blue}decaimento em direção a zero}. 
	\end{halfwideitemize}
\end{frame}
\begin{frame}{Exemplo: FAC e FACP estimadas de um MA(3) (T=10.000)}
	\begin{figure}
		\includegraphics[scale=0.33]{graficos/ma3_fac.pdf} 		\includegraphics[scale=0.33]{graficos/ma3_facp.pdf}
	\end{figure}
\end{frame}

\begin{frame}{FAC e FACP de um ARMA(p,q) estacionário e invertível}
	\begin{halfwideitemize}
		\item Generalizando a discussão anterior, um ARMA(p,q) estacionário cuja parte MA é invertível pode ser representado tanto como um $AR(\infty)$ como um $MA(\infty)$. Nesse caso, {\color{blue}tanto a FAC como a FACP apresentam decaimento}.
		\item Nesses casos, costuma-se considerar a ordem máxima $q_{\text{max}}$ em que a FAC torna-se pouco significativa e a ordem $p_{\text{max}}$ em que a FACP torna-se pouco signifcativa e considerar todos os ARMA(p,q), $0 \leq p\leq p_{\text{max}}$ e $0 \leq q \leq q_{\text{max}}$ como candidatos.
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Exemplo: FAC e FACP estimadas de um ARMA(2,3) (T=10.000)}
	\begin{figure}
		\includegraphics[scale=0.33]{graficos/arma23_fac.pdf} 		\includegraphics[scale=0.33]{graficos/arma23_facp.pdf}
	\end{figure}
\end{frame}
\begin{frame}{Resumo}
	\begin{table}[H]
		\begin{center}
			\begin{tabular}{c|c|c}
				
				Modelo& FAC & FACP   \\
				\hline
				AR(p) estacionário& decai  & truncada em $p$  \\
				\hline
				MA(q) invertível& truncada em $q$ & decai   \\
				\hline
				ARMA(p,q) estacionário  & decai  & decai  \\
				e invertível& (esp. após $q$) & (esp. após $p$)
			\end{tabular}
		\end{center}
	\end{table}
\end{frame}

\begin{transitionframe}
	\begin{center}
		{\Huge   Estimação}
	\end{center}
\end{transitionframe}

\begin{frame}{Estimação condicional vs. incondicional}
	
	\begin{halfwideitemize}
		\item Para a estimação de modelos ARMA(p,q), há duas abordagens de estimação.
		\begin{itemize}
			\item Na abordagem {\color{blue} condicional}, não utilizamos a informação acerca da distribuição das primeiras observações na estimação.
			\item Na abordagem {\color{blue} incondicional}, fazemos hipóteses adicionais sobre a distribuição do ruído branco, que nos permitem incorporar a distribuição das primeiras observações na análise.
		\end{itemize}
		\item Computacionalmente, a abordagem condicional é mais simples, embora menos eficiente que a segunda.
		\item Embora a abordagem incondicional aparente requerer mais hipóteses, visto que especificamos a distribuição do ruído branco, a estimação é robusta a violações dessa hipótese quando o número de observações é grande (``pseudo'' máxima verossimilhança).  
	\end{halfwideitemize}
	
\end{frame}

\begin{frame}{Estimação condicional do AR(p)}
	\begin{itemize}
		\item Podemos estimar os parâmetros de um AR(p) através de mínimos quadrados ordinários:
	\end{itemize}
		\begin{equation*}
			(\hat\alpha,\hat{\beta}_1,\ldots, \hat\beta_p) \in \operatorname{argmin}_{a,b_1,\ldots b_p} \frac{1}{T-p}\sum_{t=p+1}^{T}(Y_{t}- a - b_1 Y_{t-1} \ldots - b_p Y_{t-p})^2
		\end{equation*}
				\begin{itemize}
		\item Note que não tentamos prever as $p$ primeiras observações, para as quais não temos todas as defasagens.
		\item Estimadores $	(\hat\alpha,\hat{\beta}_1,\ldots, \hat\beta_p) $ coincidem com o estimador de máxima verossimilhança que usa a distribuição de $(Y_{p+1},Y_{p+2}\ldots Y_{T})$ {\color{blue}condicional} a $(Y_1,Y_2,\ldots,Y_p)$, tomando o ruído branco como $\epsilon_{it}\overset{\text{iid}}{\sim} N(0,\sigma^2)$.
		 \end{itemize}
\end{frame}
\begin{frame}{Estimação condicional do MA(1)}
	\begin{halfwideitemize}
		\item Para a estimação de um modelo MA(1) com base numa série $\{Y_{t}\}_{t=1}^T$, precisamos de um chute inicial para o ruído branco no período $t=0$.
		\item Chamemos esse chute de $\tilde{\epsilon}_0$ (padrão é tomar  $\tilde{\epsilon}_0 = 0$).

		\item Dado o chute inicial, e dado um valor candicato $c$ para o parâmetro $\theta_1$, e um valor candidato $a$ para o intercepto $\alpha$, podemos imputar o ruído branco em $t=1$:
		$$ \tilde{\epsilon}_{1}(a,c) = Y_1 - a - c \tilde{\epsilon}_0 \, .$$
		\item Procedendo recursivamente, obtemos, para todo $t\geq 2$.
		$$ \tilde{\epsilon}_{t}(a,c) = Y_t - a - c \tilde{\epsilon}_{t-1}(a,c)$$
		\item MA(1) pode ser estimado como:
		\begin{equation*}
			(\hat \alpha, \hat \theta_1) \in \operatorname{argmin}_{a, c}\frac{1}{T}\sum_{t=1}^{T}(Y_t -a  - c\tilde{\epsilon}_{t-1}(a,c))^2
		\end{equation*}
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Estimação condicional do MA(1)}
	\begin{halfwideitemize}
\item Note que $(a,c)\mapsto a + c\tilde{\epsilon}_{t-1}(a,c)$ varia não linearmente com $(a,c)$.
\begin{itemize}
	\item Estimação se dá através de algoritmos para mínimos quadrados não lineares (não há  expressão fechada para o mínimo).
\end{itemize}
\item Observe também que a primeira observação não contribui à estimação de $c$, visto que $(Y_t - a - c\tilde{\epsilon}_0)^2 = (Y_t - a)^2$.
\item Se o MA(1) é {\color{blue}invertível}, então, com $T$ grande, efeito do chute inicial sobre a função objetivo desaparece.
\begin{itemize}
	\item Contribuição do chute inicial ao erro de previsão em $t$ é da ordem de $c^t$, que desaparece quando $|c| < 1$. 
\end{itemize}
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Estimação condicional do ARMA(p,q)}
	\begin{itemize}
		\item Estendendo a discussão anterior, a estimação condicional de um ARMA(p,q) é dada pela minimização de:
\end{itemize}
		\begin{equation*}
			\small  \sum_{t= p}^{T}(Y_t -a -  b_1 Y_{t-1}  \ldots - b_p Y_{t-p} -  c_1\tilde{\epsilon}_{t-1}(a,\boldsymbol{b},\boldsymbol{c}) \ldots -c_q \tilde{\epsilon}_{t-q}(a,\boldsymbol{b},\boldsymbol{c})  )^2
\end{equation*}
onde $\tilde{\epsilon}_t(a,\boldsymbol{b},\boldsymbol{c})$ são definidos recursivamente, para cada valor candidato dos parâmetros $(a,\boldsymbol{b},\boldsymbol{c})$ e chutes iniciais dos erros $\tilde{\epsilon}_{p-q+1} = \tilde{\epsilon}_{p-q+2} = \ldots = \tilde{\epsilon}_{p} =0$ .
\begin{itemize}
	\item Perdemos $p$ observações pois não observamos os valores de $Y$ anteriores a $t=1$. Além disso, para as observações de $p+1$ a $p+q+1$, não temos informação completa para inferir todos os $\theta_j$.
\end{itemize}
\end{frame}

\begin{frame}{Estimação incondicional do ARMA(p,q)}
	\begin{itemize}
		\item Na estimação condicional, perdemos informação nas $p+q$ primeiras observações.
		\item Se fizermos uma hipótese distributiva sobre os ruídos brancos, somos capazes de caracterizar a distribuição das $p$+$q$ primeiras observações.
		\begin{itemize}
			\item Por exemplo, se $\epsilon_t \overset{\text{iid}}{\sim} N(0,\sigma^2)$ e $Y_t$ seguir um AR(1) estacionário, então $Y_1 \sim N(\alpha/(1-\alpha), \sigma^2/(1-\rho^2))$.
		\end{itemize}
		\item A estimação {\color{blue}incondicional} de um ARMA(p,q) se dá pela máxima verossimilhança que usa a distribuição conjunta de $\{Y_1,\ldots, Y_T\}$, sob a hipótese auxiliar de que  $\epsilon_t \overset{\text{iid}}{\sim} N(0,\sigma^2)$.
		\begin{itemize}
			\item Método padrão na função \texttt{arima} do \texttt{R}.
		\end{itemize}
		\item Se ruído branco de fato é Gaussiano, estimador é {\color{blue}eficiente}: dentre todos os estimadores de um ARMA(p,q) Gaussiano, estimador é o de menor variância.
		\item Mesmo que o ruído branco não seja Gaussiano, estimador ainda é {\color{blue}consistente} para os parâmetros de um ARMA(p,q).
 	\end{itemize}
\end{frame}

\begin{transitionframe}
	\begin{center}
		{\Huge   Diagnóstico}
	\end{center}
\end{transitionframe}


\begin{frame}{Diagnóstico}
	\begin{halfwideitemize}
		\item Estimados os modelos candidatos, procedemos à etapa de diagnóstico. A ideia é avaliar os modelos conforme algumas métricas:
		\begin{halfwideenumerate}
			\item Critérios de informação.
			\item Parcimônia. 
			\item Não autocorrelação dos erros.
			\item Estabilidade e invertibilidade das partes AR e MA.
			\item Convergência numérica dos estimadores.
			\item Normalidade dos erros.
		\end{halfwideenumerate}
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Critérios de informação}
	\begin{halfwideitemize}
		\item A princípio, gostaríamos de uma métrica que indicasse quanto da variabilidade do processo é explicada pelo modelo.
		\begin{halfwideitemize}
			\item Quantidade não observada; precisa ser estimada.
		\end{halfwideitemize}
		\item Intuitivamente, um estimador dessa quantidade poderia ser dado pela soma dos quadrados dos resíduos (SSR) do modelo estimado.
		\item O problema dessa métrica é que modelos mais complexos \textbf{necessariamente} apresentam SSR menor.
		\begin{halfwideitemize}
			\item Maior flexibilidade leva a melhor ajuste dentro da amostra.
			\item Isso não significa que o modelo necessariamente explique bem a variação verdadeira do processo (em particular, \textbf{fora da amostra}).
		\end{halfwideitemize}
		\item Se fôssemos escolher o modelo pelo menor SSR, sempre escolheríamos o modelo mais complexo, incorrendo num problema conhecido como sobreajuste (\textit{overfitting}).
		\begin{halfwideitemize}
			\item Modelo funcionará, em geral, muito mal fora da amostra, pois estimadores dos parâmetros apresentam alta variância.
		\end{halfwideitemize}
	\end{halfwideitemize}
\end{frame}
\begin{frame}{Critérios de informação (cont.)}
	\begin{halfwideitemize}
		\item A ideia de {\color{blue}um critério de informação} é {\color{blue}penalizar} a SSR pelo número de parâmetros estimados. 
		\begin{itemize}
			\item A penalização pode ser vista como uma forma de corrigir o viés do SSR em estimar a capacidade preditiva de um modelo.
		\end{itemize}
		\item Os critérios de informação mais utilizados são o AIC e o BIC. Para um ARMA(p,q) com intercepto, eles são dados por:
		\begin{equation*}
			\begin{aligned}
				AIC = T\text{ln}(SSR) + 2 (p+q+1) \\
				BIC = T \text{ln}(SSR) + ln(T) (p+q+1)
			\end{aligned}
		\end{equation*}
		\item Quanto menor o critério de informação, melhor.
		%\item Quando $T$ é grande, ambos os critérios são aproximadamente equivalentes.
		\item Para $T > 7$, BIC escolhe modelos com menos parâmetros.
		\item Se a estimação do modelo é condicional, importante ajustar amostra para que mesmo número de observações sejam usadas no cálculo dos critérios em todos os modelos comparados (isso já vale, por construção, na estimação incondicional).
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Parcimônia}
	\begin{halfwideitemize}
		\item Os critérios de informação induzem {\color{blue}parcimônia} no modelo escolhido, ajudando a evitar o problema de sobreajuste.
		\item Ainda sob essa lógica, é costumeiro verificar quais coeficientes são estatisticamente significantes na especificação: se houver muitos coeficientes insignificantes, talvez valha trabalhar com um modelo mais simples. 
	\end{halfwideitemize}
\end{frame}

\begin{frame}{Não autocorrelação dos erros}
	\begin{itemize}
		\item Os modelos ARMA discutidos supõem que os erros são ruído branco.
		\begin{itemize}
			\item Dessa forma, esperaríamos que os \emph{resíduos} de nosso modelo fossem aproximadamente não autocorrelacionados.
			\item Se houver correlação nos resíduos, ainda há informação nos dados que a parte sistemática do modelo não está capturando.
		\end{itemize}
		\item Teste da hipótese nula de que as autocorrelações dos erros de ordens $1$ até $s$ são zero, contra a alternativa de que ao menos uma é diferente de zero, podem ser conduzidos usando a estatística de {\color{blue}Ljung-Box}:
		\begin{equation*}
			Q = T (T+2) \sum_{j=1}^s \hat{\gamma}_j^2/(T-j) \, ,
		\end{equation*}
		onde $\hat{\gamma}_j$ é autocorrelação de ordem $j$ estimada com base nos resíduos.
		
		\item Sob a nula, distribuição de $Q$ é aproximadamente uma $\chi^2$ com $s-(p+q)$ graus de liberdade.
		\item Quanto maior $Q$, maior a evidência contra a nula. Assim, região crítica do teste é da forma $Q > c$ onde $c$ é o quantil apropriado da distribuição $\chi^2$.
	\end{itemize}
\end{frame}
\begin{frame}{Estabilidade e invertibilidade}
	\begin{halfwideitemize}
		\item Recorde-se que a análise de identificação dos modelos ARMA(p,q) pressupõe que os processos sejam estacionários e invertíveis. Assim, é costumeiro verificar se os coeficientes estimados de fato nos levam a processos estacionários e invertíveis.
		\begin{halfwideitemize}
			\item Se isso não ocorrer, devemos suspeitar de nossas estimativas.
		\end{halfwideitemize}
		\item Podemos checar a estacionariedade e invertibilidade do  ARMA(p,q) resolvendo, respectivamente, as equações de grau $p$ e $q$ dos polinômios estimados, e avaliando se as raízes se encontram todas fora do cícrulo
		
	\end{halfwideitemize}
\end{frame}
\begin{frame}{Convergência numérica}
	\begin{halfwideitemize}
		\item Os estimadores mais usados de modelos ARMA são não lineares e não possuem solução fechada.
		\begin{halfwideitemize}
			\item Por esse motivo, pacotes estatísticos usam algoritmos de otimização para estimar o modelo.  
		\end{halfwideitemize}
		\item É importante checar se os algoritmos de otimização de fato convergiram para um mínimo.
		\begin{halfwideitemize}
			\item Se esse não é o caso, devemos descartar as estimativas.
		\end{halfwideitemize}
		
	\end{halfwideitemize}
	

\end{frame}
	\begin{frame}{Normalidade dos erros}
		\begin{itemize}
			\item Recorde-se que, se os ruídos brancos forem Gaussianos, o estimador de máxima verossimilhança do ARMA(p,q) é eficiente.
			\begin{itemize}
				\item Ainda assim, mesmo que os ruídos brancos não sejam Gaussianos, o estimador é consistente.
			\end{itemize}
			\item Nesse sentido, é costumeiro testar a hipótese de normalidade dos erros de um modelo ARMA.
			\item Isso é feito verificando a assimetria e curtose dos \textit{resíduos} do modelo, e quanto elas distam do esperado em uma distribuição normal.
			\item Sob a nula de normalidade, a estatística de teste de Jarque-Bera possui distribuição $\chi^2$ com \textbf{dois} graus de liberdade.
			\begin{itemize}
				\item Valores grandes da estatística são evidência contra a nula.
			\end{itemize}
		\end{itemize}
\end{frame}


\begin{transitionframe}
	\begin{center}
		{\Huge   Previsão}
	\end{center}
\end{transitionframe}

\begin{frame}{Previsão um passo à frente}
	\begin{itemize}
		\item Estimado um ARMA(p,q) com base num conjunto de dados $\{Y_t\}_{t=1}^T$, como podemos calcular uma previsão para $Y_{T+1}$?
		\item Recorde-se que, se o processo é descrito por um ARMA(p,q), então:
		
		$$Y_{T+1} = \alpha + \sum_{j=1}^{p} \beta_j Y_{T+1-j} + \epsilon_{T+1} + \sum_{j=1}^q \theta_j \epsilon_{T+1-j} \, ,  $$
		
		\item Estimação de um ARMA nos dá estimativas para os parâmetros, além de estimativas dos ruídos brancos na janela de estimação, isto é, $\hat \epsilon_{j}$, $j \leq T$.
		\item Único componente ``desconhecido'' é $\epsilon_{T+1}$.
		\begin{itemize}
				\item Mas, como se trata de um ruído branco, um processo sem memória, o melhor a se fazer é $\hat \epsilon_{T+1} = \mathbb{E}[\epsilon_{T+1}] = 0$.
		\end{itemize}
		\item Assim, {\color{blue}a previsão um passo à frente}, com base num ARMA(p,q), é dada por:
		
		$$\hat Y_{t+1} = \hat \alpha + \sum_{j=1}^p \hat{\beta}_j Y_{T+1-j} + \sum_{j=1}^q \hat \theta_j \hat \epsilon_{T+1-j}$$
	
	\end{itemize}
\end{frame}

\begin{frame}{Previsão dois passos à frente}
	\begin{itemize}
		\item Também podemos estar interessados em prever o que ocorrerá dois passos adiante, i.e. $Y_{T+2}$.
		\item Neste caso, temos que:
		
		$$Y_{T+2} = \alpha + \sum_{j-1}^T \beta_{j} 	Y_{T+2-j} + \epsilon_{T+2} + \sum_{j=1}^q \theta_j \epsilon_{T+2-j}\, .$$
		
		\item Com base na informação até $T$, ``desconhecemos'' $\epsilon_{T+1}$, $\epsilon_{T+2}$ e $Y_{T+1}$.
		\begin{itemize}
			\item Vamos fazer $\hat \epsilon_{T+2} = \hat \epsilon_{T+1} = 0$.
			\item Vamos usar nossa previsão de $\hat{Y}_{T+1}$ para imputar $Y_{T+1}$.
		\end{itemize}
			\item Assim, {\color{blue}a previsão dois passo à frente}, com base num ARMA(p,q), é dada por:
		
		$$\hat Y_{t+1} = \hat \alpha + \hat \beta_1 \hat Y_{T+1} + \sum_{j={\color{red}2}}^p \hat{\beta}_j Y_{T+1-j} + \sum_{j={\color{red}2}}^q \hat \theta_j \hat\epsilon_{T+1-j}$$
	\end{itemize}
\end{frame}

\begin{frame}{Previsão $h$ passos à frente}
	\begin{itemize}
		\item Procedendo recursivamente, podemos definir, para qualquer $h \in \mathbb{N}$, a projeção $h$ passos à frente, $\hat Y_{t+h}$, em que usamos $\hat \epsilon_{T+j} = 0$ para $j > 0$ e as predições $\hat Y_{T+j}$, $h-1 \geq j \geq 1$, na imputação dos termos desconhecidos.
		\item Observe que para $h > \operatorname{max}(p,q)$, a projeção não usa nenhuma informação nova, somente projeções de horizontes anteriores.
		\item De fato, quando $h \to \infty$, as projeções de um ARMA(p,q) estacionário convergem para a média incondicional estimada do processo, i.e.:
		
		$$\lim_{h \to \infty} \hat Y_{T+h} = \frac{\hat \alpha}{1-\hat \beta_1  - \hat \beta_2 - \ldots - \hat \beta_p}$$
		
		\item Equivalentemente, as projeções de um ARIMA(p,1,q) convergem para projeções em que a variação é constante:
		
		$$\lim_{h \to \infty} \Delta \hat Y_{T+h} = \frac{\hat \alpha}{1-\hat \beta_1  - \hat \beta_2 - \ldots - \hat \beta_p}$$
		
	\end{itemize}
\end{frame}

\begin{frame}{Intervalo de predição}
	\begin{itemize}
		\item Um intervalo de predição para $Y_{T+h}$ com confiança $\gamma$ é um par de funções dos dados, $(L,U)$, com a propriedade:
		
		$$\mathbb{P}[L \leq Y_{T+h} \leq U] \geq \gamma\, .$$
		\item Intervalo de predição contém a realização de $Y_{T+h}$ em ao menos $100 \gamma\%$ dos casos, sobre todas as realizações possíveis da incerteza econômica.
		\item É possível construir um intervalo de predição, com base nas predições de um ARMA(p,q) Gaussiano.
		\begin{halfwideitemize}
			\item Intervalo de predição leva em conta incerteza Gaussiana acerca dos ruídos brancos, $\epsilon_{T+j}$, $j > 0$.
		\end{halfwideitemize}
		\item À medida que $h$ cresce, comprimento do intervalo de confiança cresce, visto que incerteza torna-se cada vez maior.
	\end{itemize}
	
\end{frame}

\begin{frame}{Projeções: endividamento público}\centering
\includegraphics[scale=0.9]{graficos/divida_forecast_plot.pdf}
\end{frame}

\begin{frame}{Usando projeções para comparar modelos}
\begin{itemize}
	\item Se temos uma série longa, podemos realizar um procedimento para validar e comparar os modelos, sem incorrer em problemas de sobreajuste.
	\item Ideia é repartir a amostra em dois subconjuntos de períodos, $\mathcal{E} = \{1,2,T_0\}$ e $\mathcal{V} = \{T_0+1,T_0+2,\ldots, T\}$.
	\item No conjunto $\mathcal{E}$, realizamos todas as etapas para identificar, estimar e diagnosticar modelos.
	\item Definidos um conjunto $ \mathcal{M}$ de modelos estimados com os dados em $ \mathcal{E}$, podemos calcular, para cada horizonte $h \in \mathbb{N}$, previsões $h$ passos à frente, para os períodos em $\mathcal{V}$.
	\begin{itemize}
 	\item Isto é, para cada modelo $m \in \mathcal{M}$ estimado com os dados de $\mathcal{E}$, e $t \in \mathcal{V}$, definimos como $\hat{Y}_{t+h|h}(m)$ a previsão que se faz com o modelo estimado $m$, para o período $t+h$, \textbf{com as informações até t}.
	\end{itemize}

	\item Nesse caso, podemos definir o erro quadrático médio da previsão fora da amostra, no horizonte $h$, como:
	$$\text{MSE}(m;h) =  \frac{1}{T-T_0-(h-1)}\sum_{t=T_0+h}^{T} (Y_t - \hat{Y}_{t+h|h}(m))$$
	
\end{itemize}
\end{frame}
\begin{frame}{Corrida de cavalos}
	\begin{itemize}
		\item A medida 	$\text{MSE}(m;h)$ é livre da influência de sobreajuste, visto que o modelo foi estimado sem recorrer aos dados em $\mathcal{V}$.
		\item Podemos comparar os modelos em termos de qual tem o menor $\text{MSE}(m;h)$.
		\begin{itemize}
			\item A esse procedimento damos o nome de corrida de cavalos (\textit{horseracing}).
		\end{itemize}
	\end{itemize}
\end{frame}
\end{document}